{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/usr/lib/python35.zip',\n",
       " '/usr/lib/python3.5',\n",
       " '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
       " '/usr/lib/python3.5/lib-dynload',\n",
       " '/home/anatoly/.local/lib/python3.5/site-packages',\n",
       " '/usr/local/lib/python3.5/dist-packages',\n",
       " '/home/anatoly/ParlAI',\n",
       " '/usr/local/lib/python3.5/dist-packages/numpy-1.13.1-py3.5-linux-x86_64.egg',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/anatoly/.local/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/anatoly/.ipython',\n",
       " '../../']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _pickle as pickle\n",
    "import argparse\n",
    "import json\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(word2vec_path):\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_path)\n",
    "\n",
    "    def get_word_vector(word):\n",
    "        try:\n",
    "            return model[word]\n",
    "        except KeyError:\n",
    "            return np.zeros(model.vector_size)\n",
    "\n",
    "    return get_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureDict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.load()\n",
    "        except:\n",
    "            self.feature_dict = {}\n",
    "\n",
    "    def add_data(self, data):\n",
    "        for example in data:\n",
    "            for token in example['question_tokens']+example['context_tokens']:\n",
    "                if not (token[3] == None): self.add_feature('pos='+token[3])\n",
    "                #if not (token[4] == None): self.add_feature(token[4])  # To many lemma features\n",
    "                if not (token[5] == None): self.add_feature('ner='+token[5])\n",
    "\n",
    "    def add_feature(self, feature):\n",
    "        if not self.feature_dict.get(feature):\n",
    "            self.feature_dict[feature] = len(self.feature_dict)\n",
    "\n",
    "    def _to_id(self, feature):\n",
    "        return self.feature_dict[feature]\n",
    "\n",
    "    def save(self):\n",
    "        with open('../../data/feature_dict.pkl', 'wb') as fd:\n",
    "            pickle.dump(self.feature_dict, fd)\n",
    "\n",
    "    def load(self):\n",
    "        with open('../../data/feature_dict.pkl', 'rb') as f:\n",
    "            self.feature_dict = pickle.load(f, encoding='iso-8859-1')\n",
    "\n",
    "    def renumerate(self):\n",
    "        keys = list(self.feature_dict.keys())\n",
    "        self.feature_dict = {}\n",
    "        for key in keys: self.feature_dict[key] = len(self.feature_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "\n",
    "    def __init__(self, w2v_path, extra = True, use='pos, ner, wiq, tf, is_question', use_qc = (True, False)):\n",
    "        self.word_vector = word2vec(w2v_path)\n",
    "        self.dict = FeatureDict()\n",
    "        self.use = use\n",
    "        self.extra = extra\n",
    "        self.use_qc = use_qc\n",
    "\n",
    "        keys = list(self.dict.feature_dict.keys())\n",
    "\n",
    "        if not 'pos' in use:\n",
    "            for key in keys:\n",
    "                if 'pos' in key:\n",
    "                    self.dict.feature_dict.pop(key, None)\n",
    "\n",
    "        if not 'ner' in use:\n",
    "            for key in keys:\n",
    "                if 'ner' in key:\n",
    "                    self.dict.feature_dict.pop(key, None)\n",
    "\n",
    "        self.dict.renumerate()\n",
    "\n",
    "        if 'tf' in use:\n",
    "            self.dict.add_feature('tf')\n",
    "            self.dict.add_feature('tf_rev')\n",
    "\n",
    "        if 'wiq' in use:\n",
    "            self.dict.add_feature('in_question')\n",
    "            self.dict.add_feature('in_question_uncased')\n",
    "            self.dict.add_feature('in_question_lemma')\n",
    "\n",
    "\n",
    "    def extra_features(self, sample):\n",
    "\n",
    "        context_features = np.zeros((len(sample['context_tokens']), len(self.dict.feature_dict)))\n",
    "        question_features = np.zeros((len(sample['question_tokens']), len(self.dict.feature_dict)))\n",
    "\n",
    "        def wiq(features, question=False):\n",
    "\n",
    "            if not question:\n",
    "                q_words_cased = {w for w in sample['question']}\n",
    "                q_words_uncased = {w.lower() for w in sample['question']}\n",
    "                q_lemma = {w[4] for w in sample['question_tokens']} if 'lemma' in self.use else None\n",
    "\n",
    "                for i in range(len(sample['context_tokens'])):\n",
    "                    if sample['context_tokens'][i][0] in q_words_cased:\n",
    "                        features[i][self.dict.feature_dict['in_question']] = 1.0\n",
    "                    if sample['context_tokens'][i][0].lower() in q_words_uncased:\n",
    "                        features[i][self.dict.feature_dict['in_question_uncased']] = 1.0\n",
    "                    if q_lemma and sample['context_tokens'][i] in q_lemma:\n",
    "                        features[i][self.dict.feature_dict['in_question_lemma']] = 1.0\n",
    "\n",
    "        def pos(features, question=False):\n",
    "            tokens = 'context_tokens'\n",
    "            if question:\n",
    "                tokens = 'question_tokens'\n",
    "            for i, w in enumerate(sample[tokens]):\n",
    "                f = 'pos=%s' % w[3]\n",
    "                if f in self.dict.feature_dict:\n",
    "                    features[i][self.dict.feature_dict[f]] = 1.0\n",
    "\n",
    "        def ner(features, question=False):\n",
    "            tokens = 'context_tokens'\n",
    "            if question:\n",
    "                tokens = 'question_tokens'\n",
    "            for i, w in enumerate(sample[tokens]):\n",
    "                f = 'pos=%s' % w[5]\n",
    "                if f in self.dict.feature_dict:\n",
    "                    features[i][self.dict.feature_dict[f]] = 1.0\n",
    "\n",
    "        def tf(features, question=False):\n",
    "            tokens = 'context_tokens'\n",
    "            if question:\n",
    "                tokens = 'question_tokens'\n",
    "            counter = Counter([w[0].lower() for w in sample[tokens]])\n",
    "            l = len(sample[tokens])\n",
    "            for i, w in enumerate(sample[tokens]):\n",
    "                features[i][self.dict.feature_dict['tf']] = counter[w[0].lower()] * 1.0 / l\n",
    "                features[i][self.dict.feature_dict['tf_rev']] = l / (counter[w[0].lower()] + 1.0)\n",
    "\n",
    "        if self.use_qc[0]:\n",
    "            if 'pos' in self.use:\n",
    "                pos(context_features)\n",
    "            if 'ner' in self.use:\n",
    "                ner(context_features)\n",
    "            if 'tf' in self.use:\n",
    "                ner(context_features)\n",
    "            if 'wiq' in self.use:\n",
    "                wiq(context_features)\n",
    "        else:\n",
    "            context_features = None\n",
    "\n",
    "        if self.use_qc[1]:\n",
    "            if 'pos' in self.use:\n",
    "                pos(question_features, True)\n",
    "            if 'ner' in self.use:\n",
    "                ner(question_features, True)\n",
    "            if 'tf' in self.use:\n",
    "                ner(question_features, True)\n",
    "            if 'wiq' in self.use:\n",
    "                wiq(question_features, True)\n",
    "        else:\n",
    "            question_features = None\n",
    "\n",
    "\n",
    "        return [context_features, question_features]\n",
    "\n",
    "    def to_vector(self, sample, need_answer = True):\n",
    "\n",
    "        context_vecs = [self.word_vector(token[0]) for token in sample['context_tokens']]\n",
    "        context_vecs = np.vstack(context_vecs).astype(np.float32)\n",
    "\n",
    "        question_vecs = [self.word_vector(token[0]) for token in sample['question_tokens']]\n",
    "        question_vecs = np.vstack(question_vecs).astype(np.float32)\n",
    "\n",
    "\n",
    "        if self.extra:\n",
    "            context_extra, question_exta = self.extra_features(sample)\n",
    "            if self.use_qc[0]:\n",
    "                context_vecs = np.hstack((context_vecs, context_extra))\n",
    "            if self.use_qc[1]:\n",
    "                question_vecs = np.hstack((question_vecs, question_exta))\n",
    "\n",
    "        if need_answer:\n",
    "\n",
    "            context_char_offsets = [token[2] for token in sample['context_tokens']]\n",
    "\n",
    "            try:\n",
    "                answer_start, answer_end = sample['answer_start'], sample['answer_end']\n",
    "\n",
    "                answer_start = [answer_start >= s and answer_start < e\n",
    "                                for s, e in context_char_offsets].index(True)\n",
    "                answer_end = [answer_end >= s and answer_end < e\n",
    "                              for s, e in context_char_offsets].index(True)\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "            return [[context_vecs, question_vecs], [answer_start, answer_end]]\n",
    "\n",
    "        else:\n",
    "            return [context_vecs, question_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "class Preprocessor(object):\n",
    "\n",
    "    def __init__(self, w2v_path, use, use_qc, cpus=4, need_answers=True):\n",
    "        self.cpus = cpus\n",
    "        self.use = use\n",
    "        self.w2v_path = w2v_path\n",
    "        self.use_qc = use_qc\n",
    "\n",
    "    def worker(self, arr):\n",
    "        vectorizer = Vectorizer(w2v_path=self.w2v_path, extra=False, use=self.use, use_qc=self.use_qc)\n",
    "        return [vectorizer.to_vector(sample) for sample in arr]\n",
    "\n",
    "    def preprocess(self, samples):\n",
    "        if len(samples) < 10000:\n",
    "            samples = [sample for sample in self.worker(samples) if sample is not None]\n",
    "        else:\n",
    "            chunked = chunks(samples, round(len(samples) / self.cpus))\n",
    "            p = Pool(self.cpus)\n",
    "            nested_list = p.map(self.worker, chunked)\n",
    "            samples = [val for sublist in nested_list for val in sublist if val is not None]\n",
    "\n",
    "        # Transpose\n",
    "        data = [[[], []],\n",
    "                [[], []]]\n",
    "\n",
    "        for sample in samples:\n",
    "            data[0][0].append(sample[0][0])\n",
    "            data[0][1].append(sample[0][1])\n",
    "            data[1][0].append(sample[1][0])\n",
    "            data[1][1].append(sample[1][1])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_features.ipynb\n",
      "multithreading.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsoleArgs(object):\n",
    "    def __init__(self):\n",
    "        self.word2vec_path = '../../data/word2vec_from_glove_300.vec'\n",
    "        self.outfile = '../../data/check.pkl'\n",
    "        self.data = '../../data/check_tokens.json'\n",
    "        self.use = 'pos, ner, wiq, tf'\n",
    "\n",
    "args = ConsoleArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading SQuAD data... Done!\n",
      "Making feature dict... Done!\n"
     ]
    }
   ],
   "source": [
    "    if not args.outfile.endswith('.pkl'):\n",
    "        args.outfile += '.pkl'\n",
    "\n",
    "    print('Reading SQuAD data... ', end='')\n",
    "    with open(args.data) as fd:\n",
    "        samples = json.load(fd)\n",
    "    print('Done!')\n",
    "\n",
    "    print('Making feature dict... ', end='')\n",
    "    feature_dict = FeatureDict()\n",
    "    feature_dict.add_data(samples)\n",
    "    feature_dict.save()\n",
    "    print('Done!')\n",
    "\n",
    "    try:\n",
    "        cpus = multiprocessing.cpu_count()\n",
    "    except NotImplementedError:\n",
    "        cpus = 2  # arbitrary default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5196"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'in the late 1990s',\n",
       " 'answer_end': 285,\n",
       " 'answer_start': 269,\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'context_tokens': [['When', 'When ', [0, 4], 'WRB', 'when', 'O'],\n",
       "  ['did', 'did ', [5, 8], 'VBD', 'do', 'O'],\n",
       "  ['Beyonce', 'Beyonce ', [9, 16], 'NNP', 'Beyonce', 'PERSON'],\n",
       "  ['start', 'start ', [17, 22], 'VB', 'start', 'O'],\n",
       "  ['becoming', 'becoming ', [23, 31], 'VBG', 'become', 'O'],\n",
       "  ['popular', 'popular', [32, 39], 'JJ', 'popular', 'O'],\n",
       "  ['?', '?', [39, 40], '.', '?', 'O']],\n",
       " 'id': '56be85543aeaaa14008c9063',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'question_tokens': [['When', 'When ', [0, 4], 'WRB', 'when', 'O'],\n",
       "  ['did', 'did ', [5, 8], 'VBD', 'do', 'O'],\n",
       "  ['Beyonce', 'Beyonce ', [9, 16], 'NNP', 'Beyonce', 'PERSON'],\n",
       "  ['start', 'start ', [17, 22], 'VB', 'start', 'O'],\n",
       "  ['becoming', 'becoming ', [23, 31], 'VBG', 'become', 'O'],\n",
       "  ['popular', 'popular', [32, 39], 'JJ', 'popular', 'O'],\n",
       "  ['?', '?', [39, 40], '.', '?', 'O']],\n",
       " 'topic': 'Beyoncé'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SQuAD data... "
     ]
    }
   ],
   "source": [
    "print('Processing SQuAD data... ', end='')\n",
    "prepro = Preprocessor(w2v_path=args.word2vec_path, cpus=cpus, use=args.use, use_qc=(True, True))\n",
    "data = prepro.preprocess(samples[0:5])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ..., \n",
       "          [ 0.23217   ,  0.065479  ,  0.66214001, ..., -0.34689999,\n",
       "           -0.31128001,  0.011083  ],\n",
       "          [ 0.10346   , -0.12694   ,  0.62326002, ..., -0.21291   ,\n",
       "           -0.58504999, -0.21844   ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.083992  ,  0.039025  , -0.065901  , ..., -0.35866001,\n",
       "           -0.043888  , -0.27428001],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          ..., \n",
       "          [-0.62721997,  0.11828   ,  0.70923001, ..., -0.23074999,\n",
       "           -0.15401   , -0.68997997],\n",
       "          [-0.26144999,  0.33976001, -0.095768  , ..., -0.28389001,\n",
       "           -0.17033   ,  0.1158    ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ..., \n",
       "          [ 0.36574   , -0.11894   , -0.74641001, ..., -0.37042001,\n",
       "           -0.04782   ,  0.14322001],\n",
       "          [ 0.05803   , -0.027674  , -0.23252   , ...,  0.16777   ,\n",
       "           -0.81129003,  0.093747  ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.20017   ,  0.14302   ,  0.052055  , ...,  0.034939  ,\n",
       "           -0.12599   ,  0.21863   ],\n",
       "          [-0.28657001, -0.25597   , -0.17669   , ..., -0.46542999,\n",
       "           -0.58546001,  0.17404   ],\n",
       "          ..., \n",
       "          [-1.12409997,  0.14786001, -0.16429   , ..., -0.30471   ,\n",
       "            0.22741   , -0.22732   ],\n",
       "          [-0.26144999,  0.33976001, -0.095768  , ..., -0.28389001,\n",
       "           -0.17033   ,  0.1158    ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.22232001,  0.23856001, -0.048047  , ..., -0.32701001,\n",
       "           -0.053744  , -0.41824001],\n",
       "          [ 0.20830999, -0.055975  ,  0.32861999, ..., -0.16778   ,\n",
       "            0.061694  , -0.086137  ],\n",
       "          ..., \n",
       "          [-0.0085603 ,  0.094092  ,  0.33443999, ..., -0.41503999,\n",
       "           -0.55855   ,  0.12241   ],\n",
       "          [ 0.48642001,  0.23928   , -0.082238  , ..., -0.12247   ,\n",
       "           -0.39736   ,  0.047344  ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32)],\n",
       "  [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ..., \n",
       "          [ 0.23217   ,  0.065479  ,  0.66214001, ..., -0.34689999,\n",
       "           -0.31128001,  0.011083  ],\n",
       "          [ 0.10346   , -0.12694   ,  0.62326002, ..., -0.21291   ,\n",
       "           -0.58504999, -0.21844   ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.083992  ,  0.039025  , -0.065901  , ..., -0.35866001,\n",
       "           -0.043888  , -0.27428001],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          ..., \n",
       "          [-0.62721997,  0.11828   ,  0.70923001, ..., -0.23074999,\n",
       "           -0.15401   , -0.68997997],\n",
       "          [-0.26144999,  0.33976001, -0.095768  , ..., -0.28389001,\n",
       "           -0.17033   ,  0.1158    ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.12859   ,  0.25046   , -0.55467999, ..., -0.32962999,\n",
       "            0.25841999,  0.30136001],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ..., \n",
       "          [ 0.36574   , -0.11894   , -0.74641001, ..., -0.37042001,\n",
       "           -0.04782   ,  0.14322001],\n",
       "          [ 0.05803   , -0.027674  , -0.23252   , ...,  0.16777   ,\n",
       "           -0.81129003,  0.093747  ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.20017   ,  0.14302   ,  0.052055  , ...,  0.034939  ,\n",
       "           -0.12599   ,  0.21863   ],\n",
       "          [-0.28657001, -0.25597   , -0.17669   , ..., -0.46542999,\n",
       "           -0.58546001,  0.17404   ],\n",
       "          ..., \n",
       "          [-1.12409997,  0.14786001, -0.16429   , ..., -0.30471   ,\n",
       "            0.22741   , -0.22732   ],\n",
       "          [-0.26144999,  0.33976001, -0.095768  , ..., -0.28389001,\n",
       "           -0.17033   ,  0.1158    ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32),\n",
       "   array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.22232001,  0.23856001, -0.048047  , ..., -0.32701001,\n",
       "           -0.053744  , -0.41824001],\n",
       "          [ 0.20830999, -0.055975  ,  0.32861999, ..., -0.16778   ,\n",
       "            0.061694  , -0.086137  ],\n",
       "          ..., \n",
       "          [-0.0085603 ,  0.094092  ,  0.33443999, ..., -0.41503999,\n",
       "           -0.55855   ,  0.12241   ],\n",
       "          [ 0.48642001,  0.23928   , -0.082238  , ..., -0.12247   ,\n",
       "           -0.39736   ,  0.047344  ],\n",
       "          [-0.0833    , -0.20896   , -0.043623  , ..., -0.17745   ,\n",
       "            0.055793  ,  0.80125999]], dtype=float32)]],\n",
       " [[269, 207, 526, 166, 276], [285, 225, 529, 179, 285]]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_list[2][105][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
